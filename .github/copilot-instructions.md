# GPTeasers AI Agent Instructions

## Architecture Overview

GPTeasers is a full-stack quiz generation app with a **decoupled frontend-backend architecture**:

- **Frontend**: Static HTML/CSS/JS hosted on GitHub Pages (`/frontend/`)  
- **Backend**: FastAPI service on Azure Container Apps (`/backend/`)
- **Local Development**: Docker Compose orchestrates both services

The frontend consumes **Server-Sent Events (SSE)** from the backend to stream quiz questions in real-time as they're generated by various AI providers.

## Key Components & Data Flow

### Backend (`/backend/`)
- **`fastapi_generate_quiz.py`**: Main FastAPI app with two endpoints:
  - `/GenerateQuiz`: Streams JSON quiz questions via SSE
  - `/GenerateImage`: Returns single image URL response
- **`generate_quiz.py`**: Uses `litellm` library to support multiple AI providers (OpenAI, Gemini, Azure AI, DeepSeek)
- **`response_stream_parser.py`**: Critical component that converts LLM streaming chunks into SSE format
- **`generate_image.py`**: Uses OpenAI DALL-E for image generation

### Frontend (`/frontend/scripts/`)
- **`app.js`**: Main application controller, handles button events and coordinates API calls
- **`controller.js`**: Manages API communication and EventSource for SSE consumption
- **`quiz.js`**: Quiz logic and state management
- **`ui.js`**: DOM manipulation and UI updates

## Critical Development Patterns

### AI Provider Integration
The project uses `litellm` as a universal LLM interface. When adding new providers:
1. Add to `SUPPORTED_MODELS` list in `QuizGenerator`
2. Set corresponding API keys in environment variables
3. Follow the pattern: `provider/model-name` (e.g., `"azure_ai/DeepSeek-R1"`)

### Streaming Response Handling
The `ResponseStreamParser` class is essential for converting LLM streams to SSE:
- Accumulates chunks in a buffer
- Splits on newlines to identify complete JSON objects  
- Formats as `data: {json}\n\n` for SSE protocol
- **Never modify this without understanding SSE requirements**

### Testing Strategy
Tests are split into **unit** and **integration** categories:
- **Unit tests**: Use mocks, run by default (`pytest`)
- **Integration tests**: Make real API calls, marked with `@pytest.mark.integration`
- Run integration tests: `pytest -m integration`
- Configured in `pytest.ini` to exclude integration by default

## Environment & Deployment

### Required Environment Variables
```bash
OPENAI_API_KEY=...
GEMINI_API_KEY=...
AZURE_AI_API_KEY=...
AZURE_AI_API_BASE=...
DEEPSEEK_API_KEY=...
```

### Local Development Workflow
```bash
# Full stack with Docker Compose
docker-compose up --build

# Backend only (for API development)
cd backend && uvicorn fastapi_generate_quiz:app --reload --host 0.0.0.0 --port 8000

# Access points:
# - Frontend: http://localhost:8080
# - Backend API: http://localhost:8000
```

### Deployment Architecture
- **Frontend**: Auto-deploys to GitHub Pages via `.github/workflows/static.yml` on main branch pushes
- **Backend**: Auto-deploys to Azure Container Apps via workflow trigger
- **CI**: Python linting/testing runs on backend changes via `ci_python.yml`

## Code Quality Standards

- **Linting**: Uses `ruff` for Python code formatting and linting
- **Type Checking**: Expected for new Python code
- **Testing**: Always add unit tests; integration tests for API changes
- **Dependencies**: Keep `requirements.txt` minimal; dev dependencies in `requirements-dev.txt`

## Common Development Tasks

### Adding New AI Provider
1. Update `SUPPORTED_MODELS` in `generate_quiz.py`
2. Add environment variable validation in `check_api_key_from_env()`
3. Ensure `litellm` supports the provider format
4. Add integration test for the new provider

### Modifying Quiz Format
The quiz JSON structure is defined by `example_question_1` and `example_question_2` in `QuizGenerator`. Changes here affect:
- Frontend parsing in `quiz.js`
- Backend streaming in `response_stream_parser.py`
- Test expectations

### Frontend-Backend Communication
All API calls use query parameters (not request bodies). The EventSource pattern in `controller.js` handles SSE consumption. **Never** break the SSE format when modifying backend responses.